{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Where we left off\n",
    "In Part 02 we fetched question bodies, cleaned them up a bit, and saved both the raw data and clean data to a data directory. Now we're going to analyze some of the bodies using `spacy`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spacy pipeline\n",
    "Using [`spacy`](https://spacy.io/) out-of-the-box is pretty easy. Just load in a pretrained [model](https://spacy.io/models), e.g. `\"en_core_web_sm\"` (small), `\"en_core_web_md\"` (medium), `\"en_core_web_lg\"` (large), or `\"en_core_web_trf\"` (transformer), and you're set!\n",
    "\n",
    "> NOTE: if you're having trouble downloading/installing a model try this [solution](https://stackoverflow.com/a/72636669/6509519)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T15:33:45.689796800Z",
     "start_time": "2023-07-03T15:33:43.178963700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I'm going to stay at a high level when talking about `spacy` as much as possible. But there are some terms we need to cover:\n",
    "- A [`Doc`](https://spacy.io/api/doc) object is what the `spacy` pipeline returns. It's like a string of text, but with annotations like part-of-speech.\n",
    "- A [`Token`](https://spacy.io/api/token) object is what makes up a `Doc` object. These are typically single words or punctuation marks.\n",
    "- A [`Span`](https://spacy.io/api/span) object is like a contiguous sequence of `Token` objects. When we slice a `Doc` object we get `Span` objects in return. You can think of a span as part of a sentence, like a first and last name or a phrase.\n",
    "\n",
    "\n",
    "Here is an example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a string of text.\")\n",
    "span = doc[1:3]\n",
    "token = span[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T15:42:42.421974300Z",
     "start_time": "2023-07-03T15:42:42.408633300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string of text.\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T15:42:52.765684200Z",
     "start_time": "2023-07-03T15:42:52.756684200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is a\n"
     ]
    }
   ],
   "source": [
    "print(span)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T15:43:00.204292400Z",
     "start_time": "2023-07-03T15:43:00.190862600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n"
     ]
    }
   ],
   "source": [
    "print(token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T15:43:09.445410400Z",
     "start_time": "2023-07-03T15:43:09.417887900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `Tokens` in a `Doc` have annotations added to them throughout the `nlp` [pipeline](https://spacy.io/usage/processing-pipelines). We can use these to better understand a text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts of speech: [(This, 'PRON'), (is, 'AUX'), (a, 'DET'), (string, 'NOUN'), (of, 'ADP'), (text, 'NOUN'), (., 'PUNCT')]\n",
      "Lemmas: [(This, 'this'), (is, 'be'), (a, 'a'), (string, 'string'), (of, 'of'), (text, 'text'), (., '.')]\n",
      "Is sentence start: [(This, True), (is, False), (a, False), (string, False), (of, False), (text, False), (., False)]\n"
     ]
    }
   ],
   "source": [
    "pos = [(t, t.pos_) for t in doc]\n",
    "print(f\"Parts of speech: {pos}\")\n",
    "lemma = [(t, t.lemma_) for t in doc]\n",
    "print(f\"Lemmas: {lemma}\")\n",
    "sent_start = [(t, t.is_sent_start) for t in doc]\n",
    "print(f\"Is sentence start: {sent_start}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T16:19:00.465331100Z",
     "start_time": "2023-07-03T16:19:00.440870800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `nlp` pipeline can be applied to a stream of texts using [`nlp.pipe`](https://spacy.io/api/language#pipe). This can reduce the amount of time and memory consumed during processing. Let's load in our clean data and process some of it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I recently migrated an older application we have at work from Java 1.5 to 1.6.  I noticed that during the build, I now get a (new) compiler warning:\n",
      "\n",
      "So I understand what that means, but is there a well-known alternative that is more open-standards friendly, not proprietary?  What driver do you use and/or recommend and what are the advantages of it?\n",
      "So far I have taken the approach that it compiles in 1.6, so we'll keep using it and we can find a replacement later if the next version of Java does not support it, and I will likely try to suppress the warning from showing up in the build.  Am I wrong to think that?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"../data/clean.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    # we need to parse each line individually because not every line has the same keys\n",
    "    data = [json.loads(line) for line in lines]\n",
    "# get the body of each item if it's present and convert to a doc\n",
    "docs = nlp.pipe(item.get(\"body\") for item in data if \"body\" in item)\n",
    "doc = next(docs)\n",
    "print(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T17:43:49.401801Z",
     "start_time": "2023-07-03T17:43:46.101716900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It looks like a normal chunk of text. How would we go about labeling parts as \"IS_QUESTION\" or \"NOT_QUESTION\"? We could chunk it into sentences using the [`sents`](https://spacy.io/api/doc#sents) attribute."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of detected sentences: 6\n",
      "I recently migrated an older application we have at work from Java 1.5 to 1.6.  \n",
      "------------------------------\n",
      "I noticed that during the build, I now get a (new) compiler warning:\n",
      "\n",
      "\n",
      "------------------------------\n",
      "So I understand what that means, but is there a well-known alternative that is more open-standards friendly, not proprietary?  \n",
      "------------------------------\n",
      "What driver do you use and/or recommend and what are the advantages of it?\n",
      "\n",
      "------------------------------\n",
      "So far I have taken the approach that it compiles in 1.6, so we'll keep using it and we can find a replacement later if the next version of Java does not support it, and I will likely try to suppress the warning from showing up in the build.  \n",
      "------------------------------\n",
      "Am I wrong to think that?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of detected sentences: {sum(1 for _ in doc.sents)}\")\n",
    "print(*doc.sents, sep=\"\\n\"+\"-\"*30+\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T18:10:07.103993700Z",
     "start_time": "2023-07-03T18:10:07.047359800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This first `doc` is made up of six sentences. The `\"en_core_web_sm\"` `nlp` pipeline uses a `parser` ([`DependencyParser`](https://spacy.io/api/dependencyparser)) by default, which did a pretty good job at detecting sentence boundaries. We could potentially improve it further via either the rule-based `sentencizer` ([`Sentencizer`](https://spacy.io/api/sentencizer)) or statistical-based `senter` ([`SentenceRecognizer`](https://spacy.io/api/sentencerecognizer)). We can customize the rules in the `sentencizer` or tune the already existing `senter` model with our own data.\n",
    "\n",
    "> NOTE: The `senter` component is ~10Ã— faster than the `parser` and more accurate than the rule-based `sentencizer`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of detected sentences: 6\n",
      "I recently migrated an older application we have at work from Java 1.5 to 1.6.  \n",
      "------------------------------\n",
      "I noticed that during the build, I now get a (new) compiler warning:\n",
      "\n",
      "\n",
      "------------------------------\n",
      "So I understand what that means, but is there a well-known alternative that is more open-standards friendly, not proprietary?  \n",
      "------------------------------\n",
      "What driver do you use and/or recommend and what are the advantages of it?\n",
      "\n",
      "------------------------------\n",
      "So far I have taken the approach that it compiles in 1.6, so we'll keep using it and we can find a replacement later if the next version of Java does not support it, and I will likely try to suppress the warning from showing up in the build.  \n",
      "------------------------------\n",
      "Am I wrong to think that?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# enable the `senter` model\n",
    "nlp.enable_pipe(\"senter\")\n",
    "print(f\"Number of detected sentences: {sum(1 for _ in doc.sents)}\")\n",
    "print(*doc.sents, sep=\"\\n\"+\"-\"*30+\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T18:31:29.156604200Z",
     "start_time": "2023-07-03T18:31:29.121130900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The pipeline did a pretty good job detecting (what I think are) sentences. Let's see a few more examples:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of detected sentences: 5\n",
      "I am using Ruby on Rails and I have the following Mongo collection,\n",
      "\n",
      "I have another MYSQL table called , which also has Country and  field.\n",
      "------------------------------\n",
      "I want to find the MondoDB document using  and update the value of  mongoDB  field from  table.\n",
      "------------------------------\n",
      "I want to update the country field of each document to a different values received from another MYSQL table.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "I know how to update the multiple document with the same value, but here the case is each document should be updated with different values of  field.  \n",
      "------------------------------\n",
      "I know how to perform it using looping and updating, but I want to know is there a bulk update or updateMany query available to perform this type of operation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = next(docs)\n",
    "print(f\"Number of detected sentences: {sum(1 for _ in doc.sents)}\")\n",
    "print(*doc.sents, sep=\"\\n\"+\"-\"*30+\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T18:33:12.013579300Z",
     "start_time": "2023-07-03T18:33:11.991589400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This one looks like it could be improved. Because some of the code bits were removed, the structure of the sentence is off. We humans can navigate around this, but the model struggles a bit. If I were doing this manually I'd say there are 6 sentences instead of 5 (the first sentence boundary should be split). Let's look at another."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of detected sentences: 6\n",
      "I have a Windows Forms application that displays information in a Master-Detail DataGridView, written based on the instructions at https://learn.microsoft.com/en-us/dotnet/framework/winforms/controls/create-a-master-detail-form-using-two-datagridviews.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "The data is displaying correctly, and selecting rows on the master DataGridView displays the expected data in the details DataGridView. \n",
      "\n",
      "\n",
      "------------------------------\n",
      "What I am trying to do is pass in an integer when loading the page so that the DataGridViews will display with the right master row selected and the corresponding detail rows displayed. \n",
      "\n",
      "\n",
      "------------------------------\n",
      "So far I can pass in the integer to select the correct Master row, but one still needs to click the row to display the correct details rows. \n",
      "\n",
      "\n",
      "------------------------------\n",
      "Here is the constructor for the form:\n",
      "\n",
      "\n",
      "\n",
      "In the Load() method, I populate the DGVs and Get Data for them.\n",
      "------------------------------\n",
      "Then:\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = next(docs)\n",
    "print(f\"Number of detected sentences: {sum(1 for _ in doc.sents)}\")\n",
    "print(*doc.sents, sep=\"\\n\"+\"-\"*30+\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T18:36:42.164748800Z",
     "start_time": "2023-07-03T18:36:42.139226600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks like another sentence could be split. Two out three docs look like they could have their sentence boundary detection improved. That's enough for me to start building a training data set and tune the `senter` to our needs."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
